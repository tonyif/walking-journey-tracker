---
title: "Corp QA Enterprise Standards for Scalable Systems"
description: "Enterprise-grade development standards for QA team focusing on production-ready, scalable, and maintainable systems"
tags:
  - enterprise
  - qa
  - scalability
  - production
  - best-practices
  - corp-qa
team: corp-qa
language: universal
alwaysApply: true
---

# Corp QA Enterprise Standards for Scalable Systems

## Critical Enterprise Rules

- ALWAYS follow semantic versioning (MAJOR.MINOR.PATCH)
- ALWAYS maintain comprehensive CHANGELOG.md with version history
- ALWAYS include VERSION file as single source of truth
- ALWAYS create detailed documentation (README, CONTRIBUTING, SECURITY)
- ALWAYS implement comprehensive test suite with >80% coverage
- ALWAYS use parameterized queries for database operations
- ALWAYS implement retry logic with exponential backoff
- ALWAYS log operations to files for audit trails
- ALWAYS include CI/CD pipeline integration (.gitlab-ci.yml)
- ALWAYS provide Software Bill of Materials (SBOM) for dependencies

## Documentation Standards (Beyond Generic Rules)

### Required Documentation Files

Every project MUST include:

```
project/
‚îú‚îÄ‚îÄ README.md                    # Main documentation with badges
‚îú‚îÄ‚îÄ CHANGELOG.md                 # Version history with semantic versioning
‚îú‚îÄ‚îÄ VERSION                      # Single source of truth for version
‚îú‚îÄ‚îÄ CONTRIBUTING.md              # Contribution guidelines
‚îú‚îÄ‚îÄ SECURITY.md                  # Security policy and reporting
‚îú‚îÄ‚îÄ BUILD.md                     # Build instructions for local development
‚îú‚îÄ‚îÄ TESTING.md                   # Comprehensive testing guide
‚îú‚îÄ‚îÄ QUICKSTART.md               # Quick start guide for new users
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md         # System architecture
‚îÇ   ‚îú‚îÄ‚îÄ SRD.md                  # Software Requirements Document
‚îÇ   ‚îú‚îÄ‚îÄ SDD.md                  # Software Design Document
‚îÇ   ‚îú‚îÄ‚îÄ PERFORMANCE.md          # Performance benchmarks
‚îÇ   ‚îú‚îÄ‚îÄ SBOM.json              # Software Bill of Materials
‚îÇ   ‚îî‚îÄ‚îÄ SECURITY_SCAN_RESULTS.md # Latest security audit
‚îú‚îÄ‚îÄ .gitlab-ci.yml             # CI/CD pipeline
‚îî‚îÄ‚îÄ pyproject.toml             # Modern Python project config
```

### README.md Structure

<example>
# Project Name

> One-line description

[![Version](https://img.shields.io/badge/version-1.0.0-blue)](CHANGELOG.md)
[![Pipeline Status](https://img.shields.io/badge/Pipeline-Ready-brightgreen)](pipeline-url)
[![Platform Support](https://img.shields.io/badge/Platform-Linux%20%7C%20Windows-blue)](repo-url)
[![Python](https://img.shields.io/badge/python-3.8%2B-blue)](pyproject.toml)

## Table of Contents
- [Quick Start](#quick-start)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Testing](#testing)
- [Security](#security)
- [Contributing](#contributing)
- [Changelog](#changelog)

## Quick Start (3 Steps)
1. Step 1
2. Step 2
3. Step 3

## Features
- ‚úÖ Feature 1
- ‚úÖ Feature 2
- ‚úÖ Feature 3

## Installation
### Prerequisites
### Installation Steps

## Usage
### Basic Usage
### Advanced Options

## Testing
```bash
pytest --cov=. --cov-report=html
```

## Security
See [SECURITY.md](SECURITY.md) for reporting vulnerabilities.

## Contributing
See [CONTRIBUTING.md](CONTRIBUTING.md) for contribution guidelines.

## Versioning & Changelog
Current Version: `1.0.0`
See [CHANGELOG.md](CHANGELOG.md) for version history.
</example>

## Version Management (New Standard)

### Semantic Versioning Requirements

MUST follow [Semantic Versioning 2.0.0](https://semver.org/):
- **MAJOR.MINOR.PATCH** format (e.g., 1.0.0)
- **MAJOR**: Incompatible API changes
- **MINOR**: New functionality (backwards compatible)
- **PATCH**: Bug fixes (backwards compatible)

### Version File Structure

<example>
# VERSION file (single source of truth)
1.0.0

# pyproject.toml
[project]
version = "1.0.0"

# version.py
__version__ = "1.0.0"

# installer.spec (if using PyInstaller)
VERSION = "1.0.0"
</example>

### Version Update Scripts

<example language="powershell">
# update_version.ps1
param([string]$NewVersion)

if (-not $NewVersion) {
    Write-Host "Usage: .\update_version.ps1 -NewVersion 1.1.0"
    exit 1
}

# Validate semantic version format
if ($NewVersion -notmatch '^\d+\.\d+\.\d+$') {
    Write-Host "Error: Version must be in format X.Y.Z"
    exit 1
}

# Update VERSION file
Set-Content -Path "VERSION" -Value $NewVersion

# Update pyproject.toml
(Get-Content pyproject.toml) -replace '^version = ".*"', "version = `"$NewVersion`"" | 
    Set-Content pyproject.toml

# Update version.py
(Get-Content version.py) -replace '__version__ = ".*"', "__version__ = `"$NewVersion`"" | 
    Set-Content version.py

Write-Host "‚úÖ Version updated to $NewVersion"
Write-Host "‚ö†Ô∏è  Don't forget to:"
Write-Host "   1. Update CHANGELOG.md"
Write-Host "   2. Git commit and tag: git tag v$NewVersion"
Write-Host "   3. Push tags: git push --tags"
</example>

## Testing Standards (Enhanced from Generic)

### Test Coverage Requirements

<example>
# pytest.ini
[tool:pytest]
minversion = 6.0
addopts = 
    --cov=.
    --cov-report=html
    --cov-report=term-missing
    --cov-report=xml
    --cov-fail-under=80
    --verbose
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Coverage configuration
[tool.coverage.run]
source = ["."]
omit = [
    "*/tests/*",
    "*/test_*.py",
    "*/__pycache__/*",
    "*/venv/*",
    "*/.venv/*",
    "setup.py"
]

[tool.coverage.report]
precision = 2
show_missing = true
skip_covered = false
</example>

### Test Structure

<example>
tests/
‚îú‚îÄ‚îÄ __init__.py              # Test package initialization
‚îú‚îÄ‚îÄ conftest.py              # Shared fixtures and configuration
‚îú‚îÄ‚îÄ unit/                    # Unit tests (fast, isolated)
‚îÇ   ‚îú‚îÄ‚îÄ test_utils.py
‚îÇ   ‚îú‚îÄ‚îÄ test_database.py
‚îÇ   ‚îî‚îÄ‚îÄ test_validators.py
‚îú‚îÄ‚îÄ integration/             # Integration tests (slower, real components)
‚îÇ   ‚îú‚îÄ‚îÄ test_api_integration.py
‚îÇ   ‚îú‚îÄ‚îÄ test_db_integration.py
‚îÇ   ‚îî‚îÄ‚îÄ test_pipeline.py
‚îú‚îÄ‚îÄ e2e/                     # End-to-end tests (full workflows)
‚îÇ   ‚îî‚îÄ‚îÄ test_complete_workflow.py
‚îú‚îÄ‚îÄ README.md                # Testing documentation
‚îî‚îÄ‚îÄ TESTING.md               # Comprehensive testing guide
</example>

### Test File Requirements

Every module MUST have:
1. Unit tests with >80% coverage
2. Integration tests for external dependencies
3. Parametrized tests for multiple scenarios
4. Mock fixtures for external services
5. Clear test documentation

<example>
import pytest
from unittest.mock import patch, MagicMock

class TestDriverInstaller:
    """Test suite for driver installer functionality."""
    
    @pytest.fixture
    def mock_subprocess(self):
        """Fixture for mocking subprocess calls."""
        with patch('subprocess.run') as mock:
            mock.return_value = MagicMock(returncode=0, stdout="success")
            yield mock
    
    @pytest.mark.parametrize("version,expected", [
        ("550.90", "550.90"),
        ("550.90.07", "550.90.07"),
        ("570.172.03", "570.172.03"),
    ])
    def test_version_extraction(self, version, expected):
        """Test driver version extraction from various formats."""
        url = f"http://example.com/{version}/driver.run"
        assert extract_version(url) == expected
    
    def test_install_driver_success(self, mock_subprocess):
        """Test successful driver installation."""
        result = install_driver("http://example.com/driver.run")
        assert result.success is True
        mock_subprocess.assert_called_once()
    
    def test_install_driver_failure(self, mock_subprocess):
        """Test driver installation failure handling."""
        mock_subprocess.return_value.returncode = 1
        with pytest.raises(InstallationError):
            install_driver("http://example.com/driver.run")
</example>

## Resilience & Reliability Patterns (New Standard)

### Retry Logic with Exponential Backoff

<example>
import time
import logging
from typing import Callable, Any, Optional

def retry_with_backoff(
    func: Callable,
    max_attempts: int = 3,
    initial_delay: float = 5.0,
    backoff_factor: float = 2.0,
    max_delay: float = 60.0,
    exceptions: tuple = (Exception,)
) -> Any:
    """
    Retry function with exponential backoff.
    
    Args:
        func: Function to retry
        max_attempts: Maximum retry attempts
        initial_delay: Initial delay in seconds
        backoff_factor: Multiplier for delay after each attempt
        max_delay: Maximum delay between attempts
        exceptions: Tuple of exceptions to catch
    
    Returns:
        Function return value
    
    Raises:
        Last exception if all retries fail
    """
    delay = initial_delay
    last_exception = None
    
    for attempt in range(1, max_attempts + 1):
        try:
            return func()
        except exceptions as e:
            last_exception = e
            if attempt == max_attempts:
                logging.error(
                    f"Failed after {max_attempts} attempts: {str(e)}"
                )
                raise
            
            logging.warning(
                f"Attempt {attempt}/{max_attempts} failed: {str(e)}. "
                f"Retrying in {delay:.1f}s..."
            )
            time.sleep(delay)
            delay = min(delay * backoff_factor, max_delay)
    
    raise last_exception

# Usage example
@retry_with_backoff(max_attempts=3, initial_delay=5.0)
def call_llm_api(prompt: str) -> dict:
    """Call LLM API with automatic retry."""
    response = llm_client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}]
    )
    return response
</example>

### Graceful Degradation

<example>
class ServiceWithFallback:
    """Service with fallback mechanisms."""
    
    def __init__(self, primary_url: str, fallback_url: Optional[str] = None):
        self.primary_url = primary_url
        self.fallback_url = fallback_url
        self.logger = logging.getLogger(__name__)
    
    def fetch_data(self, endpoint: str) -> dict:
        """Fetch data with fallback to secondary service."""
        try:
            return self._fetch_from_url(self.primary_url, endpoint)
        except Exception as e:
            self.logger.warning(f"Primary service failed: {e}")
            
            if self.fallback_url:
                try:
                    self.logger.info("Attempting fallback service...")
                    return self._fetch_from_url(self.fallback_url, endpoint)
                except Exception as fallback_error:
                    self.logger.error(f"Fallback also failed: {fallback_error}")
            
            # Return degraded response
            return {
                "status": "degraded",
                "error": str(e),
                "message": "Service temporarily unavailable"
            }
    
    def _fetch_from_url(self, base_url: str, endpoint: str) -> dict:
        """Internal method to fetch from URL."""
        response = requests.get(f"{base_url}/{endpoint}", timeout=30)
        response.raise_for_status()
        return response.json()
</example>

### Resume Capability

<example>
import json
from pathlib import Path
from typing import List, Dict

class ResumableProcessor:
    """Processor that can resume from where it left off."""
    
    def __init__(self, state_file: str = "processing_state.json"):
        self.state_file = Path(state_file)
        self.completed_items: set = self._load_state()
    
    def _load_state(self) -> set:
        """Load completed items from state file."""
        if self.state_file.exists():
            with open(self.state_file, 'r') as f:
                return set(json.load(f))
        return set()
    
    def _save_state(self):
        """Save completed items to state file."""
        with open(self.state_file, 'w') as f:
            json.dump(list(self.completed_items), f)
    
    def process_items(self, items: List[str], resume: bool = True):
        """
        Process items with resume capability.
        
        Args:
            items: List of items to process
            resume: Whether to skip already completed items
        """
        for item in items:
            if resume and item in self.completed_items:
                print(f"‚è≠Ô∏è  Skipping {item} (already completed)")
                continue
            
            try:
                self._process_single_item(item)
                self.completed_items.add(item)
                self._save_state()  # Save after each success
                print(f"‚úÖ Completed {item}")
            except Exception as e:
                print(f"‚ùå Failed {item}: {e}")
                # Continue processing other items
                continue
    
    def _process_single_item(self, item: str):
        """Process a single item."""
        # Implementation here
        pass
    
    def reset(self):
        """Reset processing state."""
        self.completed_items.clear()
        if self.state_file.exists():
            self.state_file.unlink()
</example>

## Security Standards (Enhanced from Generic)

### Security Documentation Requirements

<example>
# SECURITY.md structure

## üîí Security Overview
## üõ°Ô∏è Supported Versions
## üì¶ Software Bill of Materials (SBOM)
## üö® Reporting a Vulnerability
## üìÖ Response Timeline
## üéØ Severity Classification (CVSS v3.1)
## üîê Security Best Practices
## üõ†Ô∏è Known Security Considerations
## üîç Security Testing
## üì¢ Security Advisories
## üèÜ Security Hall of Fame
</example>

### SBOM Generation

<example language="python">
#!/usr/bin/env python3
"""Generate Software Bill of Materials (SBOM) for the project."""

import json
import subprocess
from datetime import datetime
from pathlib import Path

def generate_sbom(output_file: str = "docs/SBOM.json"):
    """Generate SBOM in CycloneDX 1.5 format."""
    
    # Get installed packages
    result = subprocess.run(
        ["pip", "list", "--format=json"],
        capture_output=True,
        text=True
    )
    packages = json.loads(result.stdout)
    
    # Build SBOM
    sbom = {
        "bomFormat": "CycloneDX",
        "specVersion": "1.5",
        "serialNumber": f"urn:uuid:{datetime.now().isoformat()}",
        "version": 1,
        "metadata": {
            "timestamp": datetime.now().isoformat(),
            "tools": [{
                "vendor": "Corp QA",
                "name": "SBOM Generator",
                "version": "1.0.0"
            }],
            "component": {
                "type": "application",
                "name": "your-project",
                "version": "1.0.0"
            }
        },
        "components": []
    }
    
    # Add components
    for pkg in packages:
        sbom["components"].append({
            "type": "library",
            "name": pkg["name"],
            "version": pkg["version"],
            "purl": f"pkg:pypi/{pkg['name']}@{pkg['version']}"
        })
    
    # Write SBOM
    Path(output_file).parent.mkdir(exist_ok=True)
    with open(output_file, 'w') as f:
        json.dump(sbom, f, indent=2)
    
    print(f"‚úÖ SBOM generated: {output_file}")

if __name__ == "__main__":
    generate_sbom()
</example>

### Security Scanning Integration

<example language="bash">
#!/bin/bash
# scripts/run_security_scan.sh

set -e

echo "üîí Running Security Scans..."
echo ""

# 1. Dependency Vulnerability Scanning
echo "üì¶ Scanning dependencies for known vulnerabilities..."
pip install safety pip-audit bandit
safety check --json > security-reports/safety-report.json || true
pip-audit --format json > security-reports/pip-audit-report.json || true

# 2. Code Security Scanning
echo "üîç Scanning code for security issues..."
bandit -r . -f json -o security-reports/bandit-report.json -ll || true

# 3. Secret Scanning
echo "üîë Scanning for hardcoded secrets..."
grep -r "password\s*=\s*['\"]" --include="*.py" . || echo "No hardcoded passwords found"
grep -r "api_key\s*=\s*['\"]" --include="*.py" . || echo "No hardcoded API keys found"

# 4. Generate Summary
echo ""
echo "‚úÖ Security scan complete!"
echo "üìä Reports saved to security-reports/"
echo ""
echo "Review the following:"
echo "  - security-reports/safety-report.json"
echo "  - security-reports/pip-audit-report.json"
echo "  - security-reports/bandit-report.json"
</example>

## CI/CD Pipeline Standards (New Standard)

### GitLab CI Pipeline Structure

<example>
# .gitlab-ci.yml

stages:
  - test
  - security
  - build
  - deploy

variables:
  PYTHON_VERSION: "3.8"
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"

cache:
  paths:
    - .cache/pip
    - .venv/

# Test Stage
unit-tests:
  stage: test
  image: python:${PYTHON_VERSION}
  before_script:
    - pip install -r requirements.txt
    - pip install -r requirements-dev.txt
  script:
    - pytest --cov=. --cov-report=xml --cov-report=term
    - coverage report --fail-under=80
  coverage: '/TOTAL.*\s+(\d+%)$/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml
    paths:
      - htmlcov/
    expire_in: 30 days

# Security Stage
security-scan:
  stage: security
  image: python:${PYTHON_VERSION}
  before_script:
    - pip install safety bandit pip-audit
  script:
    - safety check --json > safety-report.json || true
    - pip-audit --format json > pip-audit-report.json || true
    - bandit -r . -f json -o bandit-report.json -ll || true
  artifacts:
    paths:
      - safety-report.json
      - pip-audit-report.json
      - bandit-report.json
    expire_in: 30 days
  allow_failure: true

# Build Stage
build-executable:
  stage: build
  image: python:${PYTHON_VERSION}
  before_script:
    - pip install -r requirements.txt
    - pip install pyinstaller
  script:
    - pyinstaller installer.spec
    - ./dist/installer --version
  artifacts:
    paths:
      - dist/
    expire_in: 7 days
  only:
    - main
    - tags

# Deploy Stage
deploy-production:
  stage: deploy
  script:
    - echo "Deploying to production..."
    # Add deployment commands
  only:
    - tags
  when: manual
</example>

## Parallel Processing Standards (New Standard)

### Concurrent Processing Pattern

<example>
import concurrent.futures
import asyncio
from typing import List, Callable, Any
import logging

class ParallelProcessor:
    """Process items in parallel with configurable workers."""
    
    def __init__(self, max_workers: int = 10):
        self.max_workers = max_workers
        self.logger = logging.getLogger(__name__)
    
    def process_parallel(
        self,
        items: List[Any],
        process_func: Callable,
        use_threading: bool = False
    ) -> List[Any]:
        """
        Process items in parallel.
        
        Args:
            items: List of items to process
            process_func: Function to process each item
            use_threading: Use ThreadPoolExecutor instead of ProcessPoolExecutor
        
        Returns:
            List of results
        """
        executor_class = (
            concurrent.futures.ThreadPoolExecutor if use_threading 
            else concurrent.futures.ProcessPoolExecutor
        )
        
        results = []
        with executor_class(max_workers=self.max_workers) as executor:
            future_to_item = {
                executor.submit(process_func, item): item 
                for item in items
            }
            
            for future in concurrent.futures.as_completed(future_to_item):
                item = future_to_item[future]
                try:
                    result = future.result()
                    results.append(result)
                    self.logger.info(f"‚úÖ Completed: {item}")
                except Exception as e:
                    self.logger.error(f"‚ùå Failed {item}: {e}")
                    results.append({"error": str(e), "item": item})
        
        return results

# Async pattern for I/O-bound operations
async def process_items_async(items: List[Any], max_concurrent: int = 10):
    """Process items asynchronously with rate limiting."""
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def process_with_semaphore(item):
        async with semaphore:
            return await process_single_item(item)
    
    tasks = [process_with_semaphore(item) for item in items]
    return await asyncio.gather(*tasks, return_exceptions=True)
</example>

## Logging & Audit Trail Standards (New Standard)

### Structured Logging

<example>
import logging
import json
from datetime import datetime
from pathlib import Path

class StructuredLogger:
    """Structured logging with audit trail."""
    
    def __init__(self, log_dir: str = "logs", app_name: str = "app"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        self.app_name = app_name
        
        # Setup loggers
        self.setup_loggers()
    
    def setup_loggers(self):
        """Configure logging with multiple handlers."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Console handler (INFO and above)
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        console_handler.setFormatter(console_formatter)
        
        # File handler (DEBUG and above)
        file_handler = logging.FileHandler(
            self.log_dir / f"{self.app_name}_{timestamp}.log"
        )
        file_handler.setLevel(logging.DEBUG)
        file_formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'
        )
        file_handler.setFormatter(file_formatter)
        
        # JSON handler for structured logs
        json_handler = logging.FileHandler(
            self.log_dir / f"{self.app_name}_{timestamp}.jsonl"
        )
        json_handler.setLevel(logging.INFO)
        json_handler.setFormatter(JsonFormatter())
        
        # Configure root logger
        root_logger = logging.getLogger()
        root_logger.setLevel(logging.DEBUG)
        root_logger.addHandler(console_handler)
        root_logger.addHandler(file_handler)
        root_logger.addHandler(json_handler)

class JsonFormatter(logging.Formatter):
    """Format logs as JSON for structured logging."""
    
    def format(self, record):
        log_data = {
            "timestamp": datetime.fromtimestamp(record.created).isoformat(),
            "level": record.levelname,
            "logger": record.name,
            "function": record.funcName,
            "line": record.lineno,
            "message": record.getMessage()
        }
        
        if record.exc_info:
            log_data["exception"] = self.formatException(record.exc_info)
        
        return json.dumps(log_data)
</example>

## Performance Monitoring (New Standard)

### Performance Benchmarking

<example>
import time
import statistics
from contextlib import contextmanager
from typing import List, Dict
import logging

class PerformanceMonitor:
    """Monitor and report performance metrics."""
    
    def __init__(self):
        self.metrics: Dict[str, List[float]] = {}
        self.logger = logging.getLogger(__name__)
    
    @contextmanager
    def measure(self, operation_name: str):
        """Context manager to measure operation time."""
        start_time = time.time()
        try:
            yield
        finally:
            duration = time.time() - start_time
            if operation_name not in self.metrics:
                self.metrics[operation_name] = []
            self.metrics[operation_name].append(duration)
            self.logger.info(
                f"‚è±Ô∏è  {operation_name}: {duration:.2f}s"
            )
    
    def report_statistics(self) -> Dict[str, Dict[str, float]]:
        """Generate performance statistics report."""
        report = {}
        
        for operation, times in self.metrics.items():
            if times:
                report[operation] = {
                    "count": len(times),
                    "total": sum(times),
                    "mean": statistics.mean(times),
                    "median": statistics.median(times),
                    "min": min(times),
                    "max": max(times),
                    "stdev": statistics.stdev(times) if len(times) > 1 else 0
                }
        
        return report
    
    def print_report(self):
        """Print formatted performance report."""
        report = self.report_statistics()
        
        print("\n" + "="*80)
        print("PERFORMANCE REPORT")
        print("="*80)
        
        for operation, stats in report.items():
            print(f"\n{operation}:")
            print(f"  Count:    {stats['count']}")
            print(f"  Total:    {stats['total']:.2f}s")
            print(f"  Mean:     {stats['mean']:.2f}s")
            print(f"  Median:   {stats['median']:.2f}s")
            print(f"  Min:      {stats['min']:.2f}s")
            print(f"  Max:      {stats['max']:.2f}s")
            if stats['stdev'] > 0:
                print(f"  StdDev:   {stats['stdev']:.2f}s")

# Usage
monitor = PerformanceMonitor()

with monitor.measure("database_query"):
    # Database operation
    pass

with monitor.measure("api_call"):
    # API call
    pass

monitor.print_report()
</example>

## Database Integration Standards (New Standard)

### Hybrid Architecture Pattern

<example>
import pyodbc
import json
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime

class HybridDatabaseManager:
    """
    Hybrid database architecture with SQL Server and blob storage.
    
    - Structured data (metrics, metadata) in SQL Server
    - Unstructured data (logs, traces) in blob storage
    - Optimized for analytics and scalability
    """
    
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self.logger = logging.getLogger(__name__)
    
    def insert_test_result(
        self,
        test_name: str,
        status: str,
        metadata: Dict[str, Any],
        raw_logs: str
    ) -> int:
        """
        Insert test result using hybrid approach.
        
        Args:
            test_name: Name of the test
            status: Test status (PASS/FAIL)
            metadata: Structured metadata
            raw_logs: Raw log output (stored in blob)
        
        Returns:
            Test result ID
        """
        with pyodbc.connect(self.connection_string) as conn:
            cursor = conn.cursor()
            
            # Store structured data in SQL Server
            cursor.execute(
                """
                INSERT INTO TestResults 
                (test_name, status, duration, timestamp, metadata_json)
                VALUES (?, ?, ?, ?, ?)
                """,
                (
                    test_name,
                    status,
                    metadata.get('duration', 0),
                    datetime.now(),
                    json.dumps(metadata)
                )
            )
            
            # Get inserted ID
            cursor.execute("SELECT @@IDENTITY")
            result_id = cursor.fetchone()[0]
            
            # Store raw logs in blob reference
            # (In production, upload to Azure Blob Storage / S3)
            blob_url = self._store_logs_to_blob(result_id, raw_logs)
            
            # Update with blob reference
            cursor.execute(
                "UPDATE TestResults SET log_blob_url = ? WHERE id = ?",
                (blob_url, result_id)
            )
            
            conn.commit()
            return result_id
    
    def _store_logs_to_blob(self, result_id: int, logs: str) -> str:
        """Store logs to blob storage and return URL."""
        # Placeholder: Implement actual blob storage upload
        # For now, store locally
        log_file = f"logs/test_{result_id}.log"
        with open(log_file, 'w') as f:
            f.write(logs)
        return log_file
    
    def create_analytics_views(self):
        """Create analytics views for business intelligence."""
        with pyodbc.connect(self.connection_string) as conn:
            cursor = conn.cursor()
            
            # Flaky test identification view
            cursor.execute("""
                CREATE OR ALTER VIEW v_flaky_tests AS
                SELECT 
                    test_name,
                    COUNT(*) as run_count,
                    SUM(CASE WHEN status = 'PASS' THEN 1 ELSE 0 END) as pass_count,
                    SUM(CASE WHEN status = 'FAIL' THEN 1 ELSE 0 END) as fail_count,
                    CAST(SUM(CASE WHEN status = 'PASS' THEN 1 ELSE 0 END) AS FLOAT) / 
                        COUNT(*) as pass_rate,
                    CASE 
                        WHEN CAST(SUM(CASE WHEN status = 'PASS' THEN 1 ELSE 0 END) AS FLOAT) / 
                             COUNT(*) BETWEEN 0.3 AND 0.7 THEN 'HIGH_FLAKE'
                        WHEN CAST(SUM(CASE WHEN status = 'PASS' THEN 1 ELSE 0 END) AS FLOAT) / 
                             COUNT(*) BETWEEN 0.1 AND 0.3 THEN 'MEDIUM_FLAKE'
                        ELSE 'LOW_FLAKE'
                    END as flakiness_level
                FROM TestResults
                WHERE timestamp > DATEADD(day, -30, GETDATE())
                GROUP BY test_name
                HAVING COUNT(*) >= 5
            """)
            
            conn.commit()
            self.logger.info("‚úÖ Analytics views created")
</example>

## Project Metadata Standards (New Standard)

### pyproject.toml Structure

<example>
[project]
name = "your-project"
version = "1.0.0"
description = "Brief description"
readme = "README.md"
requires-python = ">=3.8"
authors = [
    {name = "Your Name", email = "your.email@nvidia.com"},
]
maintainers = [
    {name = "Your Name", email = "your.email@nvidia.com"},
]
keywords = ["nvidia", "qa", "automation"]
classifiers = [
    "Development Status :: 5 - Production/Stable",
    "Intended Audience :: Developers",
    "Operating System :: POSIX :: Linux",
    "Operating System :: Microsoft :: Windows",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
]
dependencies = [
    "requests>=2.31.0",
    "rich>=13.7.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=8.0.0",
    "pytest-cov>=4.1.0",
    "pytest-mock>=3.12.0",
    "black>=24.0.0",
    "pylint>=3.0.0",
    "mypy>=1.8.0",
]

[project.urls]
Homepage = "https://gitlab-master.nvidia.com/your-project"
Documentation = "https://gitlab-master.nvidia.com/your-project/-/blob/main/README.md"
Repository = "https://gitlab-master.nvidia.com/your-project"
"Issue Tracker" = "https://gitlab-master.nvidia.com/your-project/-/issues"
Changelog = "https://gitlab-master.nvidia.com/your-project/-/blob/main/CHANGELOG.md"

[project.scripts]
your-tool = "your_module:main"

[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.pytest.ini_options]
minversion = "6.0"
addopts = "--cov=. --cov-report=html --cov-report=term-missing --cov-fail-under=80"
testpaths = ["tests"]

[tool.coverage.run]
source = ["."]
omit = ["*/tests/*", "*/__pycache__/*", "*/venv/*"]

[tool.coverage.report]
precision = 2
show_missing = true
</example>

## Deployment & Release Standards (New Standard)

### Release Checklist

Before every release, verify:

- [ ] All tests pass (`pytest --cov=. --cov-fail-under=80`)
- [ ] Security scans clean (Bandit, Safety, pip-audit)
- [ ] Version updated in all files (VERSION, pyproject.toml, version.py)
- [ ] CHANGELOG.md updated with all changes
- [ ] Documentation updated (README, API docs)
- [ ] SBOM generated (`scripts/generate_sbom.py`)
- [ ] Git tag created (`git tag v1.0.0`)
- [ ] Release notes prepared
- [ ] CI/CD pipeline passes
- [ ] Stakeholders notified

### CHANGELOG.md Structure

<example>
# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Added
### Changed
### Deprecated
### Removed
### Fixed
### Security

## [1.0.0] - 2025-01-15

### Added
- Initial release
- Feature A implementation
- Feature B implementation

### Changed
- Improved performance of X by 50%

### Fixed
- Bug #123: Fixed crash on startup

### Security
- Updated dependencies to patch CVE-2024-XXXX

## [0.9.0] - 2025-01-01

### Added
- Beta release
</example>

## Industry Standards Compliance

This corp QA standard ensures compliance with:

‚úÖ **OWASP Top 10** - Web application security standards
‚úÖ **NIST Cybersecurity Framework** - Security controls
‚úÖ **Semantic Versioning 2.0.0** - Version management
‚úÖ **CycloneDX SBOM** - Software bill of materials
‚úÖ **Keep a Changelog** - Version history documentation
‚úÖ **IEEE 829** - Software test documentation
‚úÖ **ISO/IEC 25010** - Software quality standards
‚úÖ **SOC 2** - Security and compliance controls

## Summary: Key Differences from Generic Rules

| Aspect | Generic Rules | Corp QA Enhanced Standards |
|--------|--------------|---------------------------|
| Versioning | Basic mention | **Comprehensive SemVer with VERSION file** |
| Documentation | README only | **Full doc suite (11+ files)** |
| Testing | Basic unit tests | **80% coverage + integration + E2E** |
| Security | Basic guidelines | **SBOM + security scans + audit trail** |
| Resilience | Not covered | **Retry logic + graceful degradation + resume** |
| CI/CD | Not detailed | **Complete GitLab CI pipeline** |
| Logging | Basic logging | **Structured logging + audit trails** |
| Database | Basic patterns | **Hybrid architecture + analytics views** |
| Performance | General awareness | **Benchmarking + monitoring** |
| Parallel Processing | Not covered | **Concurrent/async patterns** |
| Release Process | Not formalized | **Complete release checklist** |
